experiment:
  dataset_filename: "news_category.csv"
  output_folder_name: "news_category"
  preprocessing:
    tokenization: "wordpunct"
    stemming: "wordnet"
    stopword_removal: true
    embedding_strategy: tfidf
    embedding_file: "word2vec-google-news-300"
  models:
    - model_name: cnn
      params:
        num_conv_layers: [1,2,3]
        num_conv_cells: [[128],[256,128],[512,256,128]]
        dim_filters: [[5],[5,25],[5,5,5]]
        pooling: [[5],[5,5,5],[5,5,5]]
        num_dense_layers: [1,2,3]
        num_dense_neurons: [[32],[64,32],[128,64,32]]
        optimizers: [adam, adagrad, sgd]
        epochs: [10, 20, 30]
        learning_rates: [0.001, 0.01, 0.1]
        loss: categorical_crossentropy
  evaluation:
    #k_fold:
    #k: 5
    #focus_metric: accuracy
    validation_set:
      test_size: 0.2
      validation_split: 0.2
      focus_metric: accuracy
    metrics: [ accuracy, precision, recall, f1score ]
    output:
      output_filename: results.txt